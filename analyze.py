import os
import requests
import json
import base64
import re
import time
from supabase import create_client

# ç’°å¢ƒå¤‰æ•°
SB_URL = os.environ.get("SUPABASE_URL")
SB_KEY = os.environ.get("SUPABASE_ANON_KEY")
GEMINI_KEY = os.environ.get("GEMINI_API_KEY")

supabase = create_client(SB_URL, SB_KEY)

def extract_json(text):
    try:
        match = re.search(r'\{.*\}', text, re.DOTALL)
        return json.loads(match.group()) if match else None
    except:
        return None

def analyze_batch(limit=10):
    print(f"ğŸ“‹ æœªè§£æãƒ‡ãƒ¼ã‚¿ã®æ¤œç´¢ä¸­...ï¼ˆ{limit}ä»¶ãšã¤ï¼‰")
    
    res = supabase.table("YouTubeMV_Japanese") \
        .select("video_id, thumbnail_url, title, channel_title") \
        .eq("is_analyzed", False) \
        .order("view_count", desc=True) \
        .limit(limit) \
        .execute()

    videos = res.data
    if not videos:
        return 0

    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_KEY}"
    headers = {'Content-Type': 'application/json'}

    for i, v in enumerate(videos):
        print(f"   [{i+1}/{len(videos)}] ğŸ§ {v['title']}")
        
        try:
            img_data = requests.get(v['thumbnail_url']).content
            b64_img = base64.b64encode(img_data).decode('utf-8')

            # ğŸ‘‡ ã€å¤‰æ›´ç‚¹ã€‘ã‚¿ã‚°ã®æŒ‡ç¤ºã‚’å‰Šé™¤ã—ã€åˆ¤å®šã®ã¿ã«é›†ä¸­
            prompt = (
                f"å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«: {v['title']}\n"
                f"ãƒãƒ£ãƒ³ãƒãƒ«å: {v['channel_title']}\n\n"
                "æŒ‡ç¤º:\n"
                "ã‚µãƒ ãƒã‚¤ãƒ«ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰åˆ¤æ–­ã—ã¦ã€ã“ã®å‹•ç”»ã¯ã€Œã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå…¬å¼ã®Music Videoã€ã§ã™ã‹ï¼Ÿ\n"
                "Liveæ˜ åƒã€æ­Œã£ã¦ã¿ãŸã€åˆ‡ã‚ŠæŠœãã€ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³å‹•ç”»ã¯ false ã«ã—ã¦ãã ã•ã„ã€‚\n"
                "å›ç­”ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã®ã¿ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
                "{\"is_official\": boolean, \"reason\": \"ç†ç”±ã‚’çŸ­ã\"}"
            )

            payload = {
                "contents": [{
                    "parts": [
                        {"text": prompt},
                        {"inline_data": {"mime_type": "image/jpeg", "data": b64_img}}
                    ]
                }]
            }

            response = requests.post(url, headers=headers, json=payload)
            result = response.json()

            if "error" in result:
                msg = result['error']['message']
                print(f"      âŒ APIã‚¨ãƒ©ãƒ¼: {msg}")
                print("      ğŸ§Š ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ä¸­ï¼ˆ60ç§’å¾…æ©Ÿï¼‰...")
                time.sleep(60)
                continue

            if 'candidates' in result:
                ai_text = result['candidates'][0]['content']['parts'][0]['text']
                json_data = extract_json(ai_text)
                
                if json_data:
                    # ğŸ‘‡ ã€å¤‰æ›´ç‚¹ã€‘ai_tags ã®ä¿å­˜ã‚’å‰Šé™¤
                    supabase.table("YouTubeMV_Japanese").update({
                        "is_official_mv": json_data.get("is_official", False),
                        "is_analyzed": True 
                    }).eq("video_id", v['video_id']).execute()
                    
                    print(f"      > åˆ¤å®š: {'âœ… å…¬å¼' if json_data.get('is_official') else 'âŒ å¯¾è±¡å¤–'}")
                else:
                    print(f"      âš ï¸ JSONè§£æå¤±æ•—")
            else:
                print(f"      âš ï¸ æƒ³å®šå¤–ã®ã‚¨ãƒ©ãƒ¼")

        except Exception as e:
            print(f"      âš ï¸ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")

        # ç„¡æ–™æ åˆ¶é™å›é¿ã®ãŸã‚15ç§’å¾…æ©Ÿï¼ˆå¿…é ˆï¼‰
        print("      â³ å¾…æ©Ÿä¸­(15ç§’)...")
        time.sleep(15)
    
    return len(videos)

if __name__ == "__main__":
    total_processed = 0
    while True:
        count = analyze_batch(10)
        if count == 0:
            print("\nğŸ‰ ã™ã¹ã¦ã®è§£æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            break
        total_processed += count
        print(f"ğŸµ ãƒãƒƒãƒä¼‘æ†©ä¸­... (åˆè¨ˆå®Œäº†: {total_processed}ä»¶)\n")
        time.sleep(10)
