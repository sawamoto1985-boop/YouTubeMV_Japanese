import os
import time
import re
from datetime import datetime
from googleapiclient.discovery import build
from supabase import create_client

# --- è¨­å®š ---
YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY")
SB_URL = os.environ.get("SUPABASE_URL")
SB_KEY = os.environ.get("SUPABASE_ANON_KEY")

youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
supabase = create_client(SB_URL, SB_KEY)

def is_japanese(text):
    """ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ãŒ1æ–‡å­—ã§ã‚‚å«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤å®š"""
    if not text: return False
    return bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', text))

def fetch_yearly_mvs(year, count_limit=100):
    print(f"\nğŸ“… {year}å¹´ã®MVã‚’åé›†ã—ã¦ã„ã¾ã™...")
    start_time = f"{year}-01-01T00:00:00Z"
    end_time = f"{year}-12-31T23:59:59Z"
    query = "å…¬å¼ MV -cover -æ­Œã£ã¦ã¿ãŸ -reaction -åˆ‡ã‚ŠæŠœã -LIVE -ã‚«ãƒ©ã‚ªã‚±"
    
    videos = []
    next_page_token = None
    
    while len(videos) < count_limit:
        # 1. æ¤œç´¢å®Ÿè¡Œ
        search_res = youtube.search().list(
            q=query, part="id", maxResults=50, type="video",
            videoCategoryId="10", relevanceLanguage="ja", regionCode="JP",
            publishedAfter=start_time, publishedBefore=end_time,
            order="viewCount", pageToken=next_page_token
        ).execute()
        
        v_ids = [item['id']['videoId'] for item in search_res.get('items', [])]
        if not v_ids: break

        # 2. è©³ç´°ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå†ç”Ÿæ•°ãƒ»æ¦‚è¦æ¬„ãƒ»é•·ã•ï¼‰
        details_res = youtube.videos().list(
            id=",".join(v_ids),
            part="snippet,statistics,contentDetails"
        ).execute()

        for item in details_res.get('items', []):
            snippet = item['snippet']
            stats = item.get('statistics', {})
            content_details = item.get('contentDetails', {})
            
            title = snippet['title']
            description = snippet.get('description', '')
            duration = content_details.get('duration', '')

            if is_japanese(title) or is_japanese(description):
                videos.append({
                    "video_id": item['id'],
                    "title": title,
                    "description": description[:1000],
                    "channel_title": snippet['channelTitle'],
                    "thumbnail_url": snippet['thumbnails']['high']['url'],
                    "view_count": int(stats.get('viewCount', 0)),
                    "duration": duration,
                    "published_at": snippet['publishedAt'],
                    "is_analyzed": False
                })
            
            if len(videos) >= count_limit: break
            
        next_page_token = search_res.get('nextPageToken')
        if not next_page_token: break
            
    return videos[:count_limit]

def save_to_supabase(videos):
    new_count = 0
    for v in videos:
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        check = supabase.table("YouTubeMV_Japanese").select("video_id").eq("video_id", v["video_id"]).execute()
        if not check.data:
            supabase.table("YouTubeMV_Japanese").insert(v).execute()
            new_count += 1
    print(f"  âœ… {new_count} ä»¶ä¿å­˜å®Œäº†")

if __name__ == "__main__":
    current_year = datetime.now().year
    # 2011å¹´ã‹ã‚‰ä»Šå¹´ã¾ã§ãƒ«ãƒ¼ãƒ—
    for year in range(2011, current_year + 1):
        try:
            yearly_videos = fetch_yearly_mvs(year, 100)
            save_to_supabase(yearly_videos)
            time.sleep(1) # ã“ã“ã§ã®ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã«é©åˆ‡ã«tryå†…ã«é…ç½®
        except Exception as e:
            print(f"  âŒ {year}å¹´ã®åé›†ã‚¨ãƒ©ãƒ¼: {e}")

    print("\nğŸ‰ å…¨å¹´ä»£ã®åé›†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
