import os
import time
import re
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from supabase import create_client

# ç’°å¢ƒå¤‰æ•°
YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY")
SB_URL = os.environ.get("SUPABASE_URL")
SB_KEY = os.environ.get("SUPABASE_ANON_KEY")

youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
supabase = create_client(SB_URL, SB_KEY)

def is_japanese(text):
    """ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯æ¦‚è¦æ¬„ã«æ—¥æœ¬èªãŒ1æ–‡å­—ä»¥ä¸Šå«ã¾ã‚Œã‚‹ã‹åˆ¤å®š"""
    if not text: return False
    return bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', text))

def parse_duration(duration_str):
    """ISO8601å½¢å¼(PT1M30S)ã‚’ç§’æ•°ã«å¤‰æ›"""
    m = re.search(r'(\d+)M', duration_str)
    s = re.search(r'(\d+)S', duration_str)
    h = re.search(r'(\d+)H', duration_str)
    return (int(h.group(1)) * 3600 if h else 0) + \
           (int(m.group(1)) * 60 if m else 0) + \
           (int(s.group(1)) if s else 0)

def fetch_yearly_data(year):
    print(f"\nğŸ“… {year}å¹´ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    start_time = f"{year}-01-01T00:00:00Z"
    end_time = f"{year}-12-31T23:59:59Z"
    
    # æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆAPIå´ã§ã®æ¡ä»¶è¨­å®šï¼‰
    query = '(MV "å…¬å¼") | ("Music Video" "å…¬å¼") | (MV "Official") | ("Music Video" "Official")'
    
    candidates = []
    quota_spent = 0
    
    try:
        # 1. APIæ¤œç´¢ (50ä»¶Ã—2å› = æœ€å¤§100ä»¶)
        next_page_token = None
        for _ in range(2):
            search_res = youtube.search().list(
                q=query, part="id", maxResults=50, type="video",
                videoCategoryId="10", videoDuration="medium", # éŸ³æ¥½ã‚¸ãƒ£ãƒ³ãƒ« / Shortsæ’é™¤
                relevanceLanguage="ja", regionCode="JP",
                publishedAfter=start_time, publishedBefore=end_time,
                order="viewCount", pageToken=next_page_token
            ).execute()
            quota_spent += 100
            
            ids = [item['id']['videoId'] for item in search_res.get('items', [])]
            if not ids: break
            
            # 2. å‹•ç”»è©³ç´°ã‚’ãƒãƒƒãƒå–å¾—
            details_res = youtube.videos().list(
                id=",".join(ids),
                part="snippet,statistics,contentDetails"
            ).execute()
            quota_spent += 1
            candidates.extend(details_res.get('items', []))
            
            next_page_token = search_res.get('nextPageToken')
            if not next_page_token: break

        # 3. ãƒ­ã‚¸ãƒƒã‚¯ãƒ•ã‚£ãƒ«ã‚¿ (å†ç”Ÿæ•°1ä¸‡ä»¥ä¸Š / 90ç§’ä»¥ä¸Š / æ—¥æœ¬èªå«æœ‰)
        filtered_videos = []
        seen_ids = set() # åŒä¸€ãƒãƒƒãƒå†…ã§ã®é‡è¤‡æ’é™¤ï¼ˆ21000ã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰
        
        for item in candidates:
            v_id = item['id']
            if v_id in seen_ids: continue
            
            snippet = item['snippet']
            stats = item['statistics']
            duration_sec = parse_duration(item['contentDetails']['duration'])
            view_count = int(stats.get('viewCount', 0))
            title = snippet['title']
            desc = snippet.get('description', '')

            if view_count >= 10000 and duration_sec >= 90 and is_japanese(title + desc):
                filtered_videos.append({
                    "video_id": v_id,
                    "title": title,
                    "description": desc[:1000],
                    "channel_title": snippet['channelTitle'],
                    "thumbnail_url": snippet['thumbnails']['high']['url'],
                    "view_count": view_count,
                    "duration": item['contentDetails']['duration'],
                    "published_at": snippet['publishedAt'],
                    "is_analyzed": False
                })
                seen_ids.add(v_id)

        # 4. æ›¸ãè¾¼ã¿ï¼ˆUpsertï¼‰
        new_records_count = 0
        if filtered_videos:
            # æ–°è¦è¿½åŠ åˆ†ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ãŸã‚ã®ç…§ä¼š
            target_ids = [v['video_id'] for v in filtered_videos]
            existing = supabase.table("YouTubeMV_Japanese").select("video_id").in_("video_id", target_ids).execute()
            existing_ids = {r['video_id'] for r in existing.data}
            new_records_count = len([v for v in filtered_videos if v['video_id'] not in existing_ids])

            # ãƒãƒ«ã‚¯ãƒ»ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ
            supabase.table("YouTubeMV_Japanese").upsert(filtered_videos, on_conflict="video_id").execute()

        print(f"------------------------------------")
        print(f"å¯¾è±¡: {len(candidates)} ä»¶ / æŠ½å‡º: {len(filtered_videos)} ä»¶")
        print(f"æ›¸ãè¾¼ã¿å®Ÿæ–½: {new_records_count} ä»¶ï¼ˆæ–°è¦è¿½åŠ åˆ†ï¼‰")
        print(f"æ¶ˆè²»ãƒ¦ãƒ‹ãƒƒãƒˆæ•°: {quota_spent} units")
        
        return quota_spent

    except HttpError as e:
        if e.resp.status == 403:
            print(f"âš ï¸ ã‚¯ã‚©ãƒ¼ã‚¿ä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
            return "QUOTA_EXCEEDED"
        else:
            print(f"âŒ APIã‚¨ãƒ©ãƒ¼: {e}")
            return 0

if __name__ == "__main__":
    grand_total_quota = 0
    for year in range(2011, 2027):
        result = fetch_yearly_data(year)
        
        if result == "QUOTA_EXCEEDED":
            break
        
        grand_total_quota += result
        time.sleep(1) # çŸ­æ™‚é–“ã®é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹å›é¿

    print(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ‰ å‡¦ç†ãŒçµ‚äº†ã—ã¾ã—ãŸ")
    print(f"åˆè¨ˆæ¶ˆè²»ãƒ¦ãƒ‹ãƒƒãƒˆæ•°: {grand_total_quota} units")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
