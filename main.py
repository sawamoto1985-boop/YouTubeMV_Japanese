import os
import time
import re
from googleapiclient.discovery import build
from supabase import create_client

# ç’°å¢ƒå¤‰æ•°
YT_API_KEY = os.environ.get("YOUTUBE_API_KEY")
SB_URL = os.environ.get("SUPABASE_URL")
SB_KEY = os.environ.get("SUPABASE_ANON_KEY")

supabase = create_client(SB_URL, SB_KEY)
youtube = build('youtube', 'v3', developerKey=YT_API_KEY)

# APIæ¶ˆè²»ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
total_quota_used = 0

def is_japanese(text):
    if not text: return False
    return bool(re.search(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]', text))

def get_video_stats(video_ids):
    global total_quota_used
    res = youtube.videos().list(part="statistics", id=",".join(video_ids)).execute()
    total_quota_used += 1  # videos.list ã¯ 1ãƒ¦ãƒ‹ãƒƒãƒˆ
    return {item['id']: int(item['statistics'].get('viewCount', 0)) for item in res.get('items', [])}

def fetch_and_save_mvs(target_count=1000):
    global total_quota_used
    collected_data = []
    next_page_token = None
    
    search_queries = [
        'official music video "å…¬å¼"',
        'ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ãƒ“ãƒ‡ã‚ª',
        'MV "official"',
        'é‚¦æ¥½ æœ€æ–°'
    ]
    
    print(f"ğŸš€ é‚¦æ¥½MVåé›†ï¼ˆç›®æ¨™: {target_count}ä»¶ï¼‰ã‚’é–‹å§‹ã—ã¾ã™")

    for q_text in search_queries:
        if len(collected_data) >= target_count:
            break
            
        print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {q_text}")
        next_page_token = None 

        for i in range(10): 
            if len(collected_data) >= target_count:
                break

            search_res = youtube.search().list(
                q=q_text,
                part="snippet", type="video", regionCode="JP",
                relevanceLanguage="ja", order="date", maxResults=50,
                pageToken=next_page_token
            ).execute()
            
            total_quota_used += 100  # search.list ã¯ 100ãƒ¦ãƒ‹ãƒƒãƒˆ

            items = search_res.get('items', [])
            if not items: break
            
            video_ids = [item['id']['videoId'] for item in items]
            stats_dict = get_video_stats(video_ids)

            for item in items:
                v_id = item['id']['videoId']
                snippet = item['snippet']
                
                if not (is_japanese(snippet['title']) or is_japanese(snippet['description']) or is_japanese(snippet['channelTitle'])):
                    continue

                collected_data.append({
                    "video_id": v_id,
                    "title": snippet['title'],
                    "description": snippet['description'],
                    "thumbnail_url": snippet['thumbnails']['high']['url'],
                    "published_at": snippet['publishedAt'],
                    "channel_title": snippet['channelTitle'],
                    "view_count": stats_dict.get(v_id, 0),
                    "is_analyzed": False
                })

            next_page_token = search_res.get('nextPageToken')
            print(f"ğŸ“ˆ ç´¯è¨ˆå–å¾—: {len(collected_data)}ä»¶ / æ¶ˆè²»API: {total_quota_used}ãƒ¦ãƒ‹ãƒƒãƒˆ")
            
            if not next_page_token: break
            time.sleep(0.1)

    # Supabaseã¸ä¸€æ‹¬ä¿å­˜
    if collected_data:
        unique_data = list({v['video_id']: v for v in collected_data}.values())[:target_count]
        for i in range(0, len(unique_data), 100):
            batch = unique_data[i:i+100]
            supabase.table("YouTubeMV_Japanese").upsert(batch).execute()
        
        print("-" * 30)
        print(f"âœ… æœ€çµ‚çµæœ: {len(unique_data)}ä»¶ã‚’åŒæœŸå®Œäº†")
        print(f"ğŸ“Š æœ¬æ—¥ã®ç·æ¶ˆè²»API: {total_quota_used} ãƒ¦ãƒ‹ãƒƒãƒˆ")
        print(f"ğŸ’¡ æ®‹ã‚Šæ¨å®š: {10000 - total_quota_used} ãƒ¦ãƒ‹ãƒƒãƒˆ (ç„¡æ–™æ å†…)")
        print("-" * 30)

if __name__ == "__main__":
    fetch_and_save_mvs(1000)
