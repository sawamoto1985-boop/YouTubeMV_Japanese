import os
import time
import re
from datetime import datetime
from googleapiclient.discovery import build
from supabase import create_client

# --- è¨­å®š ---
YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY")
SB_URL = os.environ.get("SUPABASE_URL")
SB_KEY = os.environ.get("SUPABASE_ANON_KEY")

youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
supabase = create_client(SB_URL, SB_KEY)

def is_japanese(text):
    """ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ãŒ1æ–‡å­—ã§ã‚‚å«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤å®š"""
    if not text:
        return False
    # Unicodeã®ç¯„å›²: ã²ã‚‰ãŒãª(\u3040-\u309F)ã€ã‚«ã‚¿ã‚«ãƒŠ(\u30A0-\u30FF)ã€æ¼¢å­—(\u4E00-\u9FFF)
    return bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', text))

def fetch_yearly_mvs(year, count_limit=100):
    print(f"\nğŸ“… {year}å¹´ã®MVã‚’åé›†ã—ã¦ã„ã¾ã™...")
    
    start_time = f"{year}-01-01T00:00:00Z"
    end_time = f"{year}-12-31T23:59:59Z"
    
    # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã€Œå…¬å¼ã€ãªã©ã®æ—¥æœ¬èªä¸»ä½“ã«
    query = "å…¬å¼ MV -cover -æ­Œã£ã¦ã¿ãŸ -reaction -åˆ‡ã‚ŠæŠœã -LIVE -ã‚«ãƒ©ã‚ªã‚±"
    
    videos = []
    next_page_token = None
    
    # æŒ‡å®šä»¶æ•°ã«é”ã™ã‚‹ã‹ã€æ¤œç´¢çµæœãŒå°½ãã‚‹ã¾ã§ãƒ«ãƒ¼ãƒ—
    while len(videos) < count_limit:
        search_response = youtube.search().list(
            q=query,
            part="snippet",
            maxResults=50,
            type="video",
            videoCategoryId="10",
            relevanceLanguage="ja",
            regionCode="JP",
            publishedAfter=start_time,
            publishedBefore=end_time,
            order="viewCount",
            pageToken=next_page_token
        ).execute()
        
        for item in search_response['items']:
            snippet = item['snippet']
            title = snippet['title']
            description = snippet['description']
            
            # ã€é‡è¦ã€‘æ—¥æœ¬èªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼šã‚¿ã‚¤ãƒˆãƒ«ã‹æ¦‚è¦æ¬„ã«æ—¥æœ¬èªãŒã‚ã‚Œã°OK
            if is_japanese(title) or is_japanese(description):
                videos.append({
                    "video_id": item['id']['videoId'],
                    "title": title,
                    "channel_title": snippet['channelTitle'],
                    "thumbnail_url": snippet['thumbnails']['high']['url'],
                    "published_at": snippet['publishedAt'],
                    "view_count": 0,
                    "is_analyzed": False
                })
            
            if len(videos) >= count_limit:
                break
            
        next_page_token = search_response.get('nextPageToken')
        if not next_page_token:
            break
            
    return videos[:count_limit]

def save_to_supabase(videos):
    new_count = 0
    for v in videos:
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        check = supabase.table("YouTubeMV_Japanese").select("video_id").eq("video_id", v["video_id"]).execute()
        
        if not check.data:
            supabase.table("YouTubeMV_Japanese").insert(v).execute()
            new_count += 1
            
    print(f"  âœ… {new_count} ä»¶ã®å›½å†…å‘ã‘å‹•ç”»ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    # SQLã§TRUNCATEã—ãŸå¾Œã€ã“ã‚Œã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„
    current_year = datetime.now().year
    for year in range(2011, current_year + 1):
        try:
            yearly_videos = fetch_yearly_mvs(year, 100)
            save_to_supabase(yearly_videos)
            time.sleep(1) # APIåˆ¶é™ã«é…æ…®
        except Exception as e:
            print(f"  âŒ {year}å¹´ã®åé›†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    print("\nğŸ‰ å›½å†…å‘ã‘MVã®åé›†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
