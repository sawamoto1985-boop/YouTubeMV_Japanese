import os
import time
import re
from googleapiclient.discovery import build
from supabase import create_client

# ç’°å¢ƒå¤‰æ•°
YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY")
SB_URL = os.environ.get("SUPABASE_URL")
SB_KEY = os.environ.get("SUPABASE_ANON_KEY")

youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
supabase = create_client(SB_URL, SB_KEY)

def is_japanese(text):
    if not text: return False
    return bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', text))

def parse_duration(duration_str):
    """ISO8601(PT1M30S)ã‚’ç§’æ•°ã«å¤‰æ›"""
    m = re.search(r'(\d+)M', duration_str)
    s = re.search(r'(\d+)S', duration_str)
    return (int(m.group(1)) * 60 if m else 0) + (int(s.group(1)) if s else 0)

def fetch_yearly_data(year):
    print(f"\nğŸ“… {year}å¹´ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    start_time = f"{year}-01-01T00:00:00Z"
    end_time = f"{year}-12-31T23:59:59Z"
    
    # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã¿åˆã‚ã› (APIæ¡ä»¶)
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è€ƒæ…®ã—ãŸAND/ORã‚¯ã‚¨ãƒª
    query = '(MV "å…¬å¼") | ("Music Video" "å…¬å¼") | (MV "Official") | ("Music Video" "Official")'
    
    candidates = []
    total_quota = 0
    
    # APIæ¤œç´¢ (50ä»¶Ã—2å› = 100ä»¶)
    next_page_token = None
    for _ in range(2):
        search_res = youtube.search().list(
            q=query, part="id", maxResults=50, type="video",
            videoCategoryId="10", videoDuration="medium", # 1. éŸ³æ¥½ã‚¸ãƒ£ãƒ³ãƒ« / 4. Shortsæ’é™¤
            relevanceLanguage="ja", regionCode="JP", # 3. æ—¥æœ¬å‘ã‘
            publishedAfter=start_time, publishedBefore=end_time,
            order="viewCount", pageToken=next_page_token
        ).execute()
        total_quota += 100
        
        ids = [item['id']['videoId'] for item in search_res.get('items', [])]
        if not ids: break
        
        # è©³ç´°æƒ…å ±ã‚’ãƒãƒƒãƒå–å¾— (1å›ã§50ä»¶åˆ†)
        details_res = youtube.videos().list(
            id=",".join(ids),
            part="snippet,statistics,contentDetails"
        ).execute()
        total_quota += 1
        
        candidates.extend(details_res.get('items', []))
        next_page_token = search_res.get('nextPageToken')
        if not next_page_token: break

    # --- ãƒ­ã‚¸ãƒƒã‚¯ãƒ•ã‚£ãƒ«ã‚¿ ---
    filtered_videos = []
    for item in candidates:
        snippet = item['snippet']
        stats = item['statistics']
        duration_str = item['contentDetails']['duration']
        
        view_count = int(stats.get('viewCount', 0))
        duration_sec = parse_duration(duration_str)
        title = snippet['title']
        desc = snippet.get('description', '')

        # ãƒ­ã‚¸ãƒƒã‚¯æ¡ä»¶: å†ç”Ÿæ•°1ä¸‡ä»¥ä¸Š / 90ç§’ä»¥ä¸Š / æ—¥æœ¬èªå«æœ‰
        if view_count >= 10000 and duration_sec >= 90 and is_japanese(title + desc):
            filtered_videos.append({
                "video_id": item['id'],
                "title": title,
                "description": desc[:1000],
                "channel_title": snippet['channelTitle'],
                "thumbnail_url": snippet['thumbnails']['high']['url'],
                "view_count": view_count,
                "duration": duration_str,
                "published_at": snippet['publishedAt'],
                "is_analyzed": False
            })

    # --- æ›¸ãè¾¼ã¿å‡¦ç† (Upsert) ---
    new_records_count = 0
    if filtered_videos:
        # æ—¢å­˜IDã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æ–°è¦è¿½åŠ åˆ†ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        target_ids = [v['video_id'] for v in filtered_videos]
        existing = supabase.table("YouTubeMV_Japanese").select("video_id").in_("video_id", target_ids).execute()
        existing_ids = {r['video_id'] for r in existing.data}
        new_records_count = len([v for v in filtered_videos if v['video_id'] not in existing_ids])

        # ãƒãƒ«ã‚¯ãƒ»ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆå®Ÿè¡Œ (ä¸€å›ã®é€šä¿¡ã§å®Œäº†)
        supabase.table("YouTubeMV_Japanese").upsert(filtered_videos, on_conflict="video_id").execute()

    # ãƒ­ã‚°å‡ºåŠ›
    print(f"------------------------------------")
    print(f"å¯¾è±¡: {len(candidates)} ä»¶")
    print(f"æŠ½å‡º: {len(filtered_videos)} ä»¶ï¼ˆãƒ•ã‚£ãƒ«ã‚¿é€šéï¼‰")
    print(f"æ›¸ãè¾¼ã¿å®Ÿæ–½: {new_records_count} ä»¶ï¼ˆæ–°è¦è¿½åŠ åˆ†ï¼‰")
    print(f"æ¶ˆè²»ãƒ¦ãƒ‹ãƒƒãƒˆæ•°: {total_quota} units")
    
    return total_quota

if __name__ == "__main__":
    grand_total_quota = 0
    # 15å¹´å‰(2011)ã‹ã‚‰ç¾åœ¨(2026)ã¾ã§
    for year in range(2011, 2027):
        try:
            grand_total_quota += fetch_yearly_data(year)
            time.sleep(1) # å›ç·šè² è·è»½æ¸›
        except Exception as e:
            print(f"  âŒ {year}å¹´ã®ã‚¨ãƒ©ãƒ¼: {e}")

    print(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ‰ å…¨å¹´ä»£ã®åé›†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"ãƒˆãƒ¼ã‚¿ãƒ«ã®æ¶ˆè²»ãƒ¦ãƒ‹ãƒƒãƒˆæ•°: {grand_total_quota} units")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
